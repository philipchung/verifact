## Environment Configuration File

# Secrets
HF_TOKEN=""

## Local Environment Configuration
SERVER_NAME=localhost

# Paths
PROJECT_DIR=${HOME}/verifact

# Source Data Directories
DATA_DIR=${PROJECT_DIR}/data
PHYSIONET_DIR=${DATA_DIR}/physionet.org
MIMIC3_DIR=${DATA_DIR}/physionet.org/files/mimiciii/1.4
DATASET_DIR=${DATA_DIR}/dataset
VERIFACTBHC_DATASET_DIR=${DATA_DIR}/physionet.org/files/mimic-iii-ext-verifact-bhc/1.0.0
VERIFACTBHC_PROPOSITIONS_DIR=${VERIFACTBHC_DATASET_DIR}/propositions
VERIFACTBHC_EHR_DIR=${VERIFACTBHC_DATASET_DIR}/reference_ehr

# VeriFact Result Directories
VERIFACT_RESULTS_DIR=${DATA_DIR}/verifact_results

# Ingested Data Directories
STORAGE_DIR=${PROJECT_DIR}/storage
QDRANT_STORAGE_PATH=${STORAGE_DIR}/qdrant
REDIS_STORAGE_PATH=${STORAGE_DIR}/redis

# Qdrant Collection Names
MIMIC3_EHR_COLLECTION_NAME=ehr_noteevents
MIMIC3_HC_COLLECTION_NAME=bhc_noteevents
MIMIC3_LLM_HC_COLLECTION_NAME=llm_bhc_noteevents

# HuggingFace Configs
TOKENIZERS_PARALLELISM=false
HF_HOME=${HOME}/.cache/huggingface
HF_HUB_CACHE=${HF_HOME}

# Apprise Notification Webhooks
NOTIFY_WEBHOOK_URL=""

## Docker & Traefik Configuration

# Service Hosts, Ports, URLs
TRAEFIK_HOST=traefik.localhost
TRAEFIK_CONTAINER_PORT=8080
TRAEFIK_HOST_PORT=8090
TRAEFIK_URL=http://${TRAEFIK_HOST}:${TRAEFIK_CONTAINER_PORT}/
TRAEFIK_DASHBOARD_URL=http://${TRAEFIK_HOST}:${TRAEFIK_HOST_PORT}/dashboard/

LLM_HOST=llm.localhost
LLM_CONTAINER_PORT=8000
LLM0_HOST_PORT=8100
LLM1_HOST_PORT=8101
LLM2_HOST_PORT=8102
LLM3_HOST_PORT=8103
LLMTP2_HOST_PORT=8104
LLM_URL_BASE=http://${LLM_HOST}/v1/
LLM_URL=http://${LLM_HOST}/v1/

AUX_LLM_HOST=aux-llm.localhost
AUX_LLM_HOST_PORT=8105
AUX_LLM_URL_BASE=http://${AUX_LLM_HOST}/v1/
AUX_LLM_URL=http://${AUX_LLM_HOST}/v1/

EMBED_HOST=embed.localhost
EMBED_CONTAINER_PORT=7997
EMBED0_HOST_PORT=8110
EMBED1_HOST_PORT=8111
EMBED2_HOST_PORT=8112
EMBED3_HOST_PORT=8113
EMBED_URL_BASE=http://${EMBED_HOST}/v1
EMBED_URL=http://${EMBED_HOST}/v1/embeddings/

RERANK_HOST=rerank.localhost
RERANK_CONTAINER_PORT=7997
RERANK0_HOST_PORT=8120
RERANK1_HOST_PORT=8121
RERANK2_HOST_PORT=8122
RERANK3_HOST_PORT=8123
RERANK_URL_BASE=http://${RERANK_HOST}/v1
RERANK_URL=http://${RERANK_HOST}/v1/rerank/

REDIS_HOST=redis.localhost
REDIS_CONTAINER_PORT=6379
REDIS_HOST_PORT=6379
REDIS_URL=redis://${REDIS_HOST}:${REDIS_HOST_PORT}/
REDIS_DASHBOARD_CONTAINER_PORT=8001
REDIS_DASHBOARD_HOST_PORT=6380
REDIS_DASHBOARD_URL=http://${REDIS_HOST}:${REDIS_DASHBOARD_HOST_PORT}/

RQ_DASHBOARD_HOST=rq-dashboard.localhost
RQ_DASHBOARD_CONTAINER_PORT=9181
RQ_DASHBOARD_HOST_PORT=9181
RQ_DASHBOARD_URL=http://${RQ_DASHBOARD_HOST}:${RQ_DASHBOARD_HOST_PORT}/

QDRANT_HOST=qdrant.localhost
QDRANT_CONTAINER_PORT=6333
QDRANT_HOST_PORT=6333
QDRANT_GRPC_CONTAINER_PORT=6334
QDRANT_GRPC_HOST_PORT=6334
QDRANT_URL=http://${QDRANT_HOST}:${QDRANT_HOST_PORT}/
QDRANT_DASHBOARD_URL=http://${QDRANT_HOST}:${QDRANT_HOST_PORT}/dashboard/

## Prometheus & Grafana
MONITORING_CONFIG_DIR=${PROJECT_DIR}/services/vllm/monitoring
PROMETHEUS_CONTAINER_PORT=9090
PROMETHEUS_HOST_PORT=9090

GRAFANA_CONTAINER_PORT=3000
GRAFANA_HOST_PORT=3000

## Docker Environment
# Traefik settings
ADMIN_EMAIL=email@domain.edu # e.g. ADMIN_EMAIL=admin@website.com
DOMAIN=localhost # e.g. DOMAIN=website.com
CERT_RESOLVER= #letsencrypt # keep it blank for internal private or local net
TRAEFIK_USER=admin
TRAEFIK_PASSWORD_HASH=verifact

# Embed Model
EMBED_MODEL_NAME=BAAI/bge-m3

# Rerank Model
RERANK_MODEL_NAME=BAAI/bge-reranker-v2-m3


## vLLM settings (https://docs.vllm.ai/en/latest/models/engine_args.html)

### Non-reasoning Models
## Llama-3.1-8B
# LLM_MODEL_NAME=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
# TOKENIZER_MODEL_NAME=meta-llama/Meta-Llama-3.1-8B-Instruct
## Llama-3.1-70B
LLM_MODEL_NAME=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4
TOKENIZER_MODEL_NAME=meta-llama/Meta-Llama-3.1-70B-Instruct

### Reasoning Models
## DeepSeek-R1-Distill-Llama-8B
# LLM_MODEL_NAME=casperhansen/deepseek-r1-distill-llama-8b-awq
# TOKENIZER_MODEL_NAME=deepseek-ai/DeepSeek-R1-Distill-Llama-8B
## DeepSeek-R1-Distill-Llama-70B
# LLM_MODEL_NAME=casperhansen/deepseek-r1-distill-llama-70b-awq
# TOKENIZER_MODEL_NAME=deepseek-ai/DeepSeek-R1-Distill-Llama-70B

DTYPE=auto # NOTE: Use float16 if quantized; otherwise use bfloat16
LLM_MAX_MODEL_LEN=20000
# Tensor Parallel GPU Selection When Running vLLM with Tensor Parallelism = 2
TP2_GPU0=2 # Set to 0,1,2,3 to specify GPU device to make available to Docker Container
TP2_GPU1=3
GPU_MEMORY_UTILIZATION=0.90
GPU0_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION}
GPU1_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION}
GPU2_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION}
GPU3_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION}
BLOCK_SIZE=16
SWAP_SPACE=4 # GiB
GUIDED_DECODING_BACKEND=xgrammar

# Non-reasoning Model Extra Arguments
EXTRA_ARGS="--enable-prefix-caching --num-scheduler-steps=1 --max-num-seqs=512"
# Reasoning Model Extra Arguments
# EXTRA_ARGS="--enable-prefix-caching --num-scheduler-steps=1 --max-num-seqs=512 --enable-reasoning --reasoning-parser=deepseek_r1"

## vLLM Auxillary (for Structured Outputs) Service Settings 
AUX_LLM_MODEL_NAME=hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4
AUX_TOKENIZER_MODEL_NAME=meta-llama/Meta-Llama-3.1-8B-Instruct

# GPU for Auxillary LLM 
AUX_LLM_GPU=1 # Set to 0,1,2,3 to specify GPU device to make available to Docker Container
AUX_DTYPE=auto
AUX_LLM_MAX_MODEL_LEN=6000
AUX_LLM_GPU_MEMORY_UTILIZATION=0.2
AUX_BLOCK_SIZE=16
AUX_SWAP_SPACE=4 # GiB
AUX_GUIDED_DECODING_BACKEND=xgrammar
AUX_EXTRA_ARGS="--enable-prefix-caching --num-scheduler-steps=1 --max-num-seqs=256"
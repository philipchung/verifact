from collections.abc import Callable
from typing import Any, Protocol, runtime_checkable

from llama_index.core.schema import NodeWithScore


@runtime_checkable
class FusionCallable(Protocol):
    """Fusion callable protocol."""

    def __call__(
        self,
        results: dict[str, list[NodeWithScore]],
        **kwargs: Any,
    ) -> list[NodeWithScore]:
        """Fusion callable. `results` contains arbitrary number of result lists of NodeWithScore
        objects which are fused and returned as a list of NodesWithSCore"""
        ...


def save_score_to_node_metadata(results: dict[str, list[NodeWithScore]]) -> list[NodeWithScore]:
    """Save node scores to node metadata.

    The `score` property will be overwritten when new scores or ranks are calculated.
    Calling this function will save the original scores to node metadata to preserve
    the original scores for downstream analysis.

    Args:
        results (Dict[str, List[NodeWithScore]]): Results from multiple
            queries and/or retrievers that we want to fuse.
            Keys identify a named result of nodes generated from a query and retriever.

    Returns:
        List[NodeWithScore]: Original list with scores. Nodes have scores saved to metadata.
    """
    # Add scores from each retriever/result list to node metadata
    for result_name, nodes_with_scores in results.items():
        for node_with_score in nodes_with_scores:
            node_with_score.node.metadata[f"{result_name}"] = node_with_score.score
    return results


def simple_fusion(
    results: dict[str, list[NodeWithScore]],
    save_score: bool | str = False,
    format_metadata_key: Callable[[str], str] | None = None,
) -> list[NodeWithScore]:
    """Apply simple fusion. This simply combines both lists and deduplicates common nodes.

    Based on llama_index.core.retrievers.fusion_retriever.QueryFusionRetriever

    Args:
        results (Dict[str, List[NodeWithScore]]): Results from multiple
            queries and/or retrievers that we want to fuse.
            Keys identify a named result of nodes generated from a query and retriever.
        save_score (bool, str, optional): If true, saves original scores to node metadata.
        format_metadata_key (Callable, optional): A function that takes a string and
            returns a string. If provided, will be used to format the string key in
            results which will become the key in the node metadata dict. Defaults to None.

    Returns:
        List[NodeWithScore]: List with node scores generated by fusion method.
    """
    ## Save Original Scores
    if save_score:
        r = results
        if format_metadata_key:
            r = {format_metadata_key(k): v for k, v in r.items()}
        r = {f"{k}_score": v for k, v in r.items()}
        save_score_to_node_metadata(r)

    # Use a dict to de-duplicate nodes
    all_nodes: dict[str, NodeWithScore] = {}

    for nodes_with_scores in results.values():
        for node_with_score in nodes_with_scores:
            text = node_with_score.node.get_content()
            if text in all_nodes:
                max_score = max(node_with_score.score, all_nodes[text].score)
                all_nodes[text].score = max_score
            else:
                all_nodes[text] = node_with_score

    return sorted(all_nodes.values(), key=lambda x: x.score or 0.0, reverse=True)


def relative_score_fusion(
    results: dict[str, list[NodeWithScore]],
    weights: dict[str, float] | None = None,
    dist_based: bool | None = False,
    save_score: bool = False,
    format_metadata_key: Callable[[str], str] | None = None,
) -> list[NodeWithScore]:
    """Apply relative score fusion.

    Based on llama_index.core.retrievers.fusion_retriever.QueryFusionRetriever

    Args:
        results (Dict[str], List[NodeWithScore]]): Results from multiple
            queries and/or retrievers that we want to fuse.
            Keys identify a named result of nodes generated from a query and retriever.
            Values are lists of nodes with scores for that query and retriever.
        weights (Dict[str, float], optional): Weights for each retriever and the
            corresponding retrieved named result list. Defaults to None. If None,
            then all retrievers are weighted equally.
        dist_based (Optional[bool], optional): If true, computes distribution-based score fusion.
            If false, computes min-max normalization score fusion (aka relative score fusion).
            Defaults to False.
        save_score (bool, optional): If true, saves original scores to node metadata.
            By default the vector name is used for the metadata key and the normalized score
            is saved as the vector name with "_normalized" appended. Vector name can be
            changed using `format_metadata_key` argument. Defaults to False.
        format_metadata_key (Callable, optional): A function that takes a string and
            returns a string. If provided, will be used to format the string key in
            results which will become the key in the node metadata dict. If None, will take
            the string key in results and append "_score" to use as the metadata key.
            Defaults to None.

    Returns:
        List[NodeWithScore]: List with node scores generated by fusion method.
        NOTE: The final fused scores are NOT normalized between 0-1. Each retriever's
        scores are normalized between 0-1 and then scaled by the weight of the retriever.
        Nodes
    """
    ## Save Original Scores
    if save_score:
        r = results
        if format_metadata_key:
            r = {format_metadata_key(k): v for k, v in r.items()}
        r = {f"{k}_score": v for k, v in r.items()}
        save_score_to_node_metadata(r)

    ## Weighting for each retriever (result list)
    if weights is not None:
        # If custom weighing of result lists, scale sum of retriever weights to 1
        total_weight = sum(weights.values())
        if weights != 1.0:
            weights = {result_name: w / total_weight for result_name, w in weights.items()}

    ## Normalize Scores for each result list (one result list per retriever)
    for result_name, nodes_with_scores in results.items():
        scores = [node_with_score.score for node_with_score in nodes_with_scores]
        if scores:
            # Identify minimum & maximum score range
            if dist_based:
                # Set min and max based on mean and std dev
                mean_score = sum(scores) / len(scores)
                std_dev = (sum((x - mean_score) ** 2 for x in scores) / len(scores)) ** 0.5
                min_score = mean_score - 3 * std_dev
                max_score = mean_score + 3 * std_dev
            else:
                min_score = min(scores)
                max_score = max(scores)

            # Scale scores of each retrieved node to be between 0 and 1
            # Then scale by the weight of the retriever
            for node_with_score in nodes_with_scores:
                # Scale the score to be between 0 and 1
                if max_score == min_score:
                    node_with_score.score = 1.0 if max_score > 0 else 0.0
                else:
                    node_with_score.score = (node_with_score.score - min_score) / (
                        max_score - min_score
                    )
                # Scale by the weight of the retriever
                if weights is not None:
                    node_with_score.score *= weights[result_name]

    ## Save Normalized Scores
    if save_score:
        r = results
        if format_metadata_key:
            r = {format_metadata_key(k): v for k, v in r.items()}
        r = {f"{k}_normalized_score": v for k, v in r.items()}
        save_score_to_node_metadata(r)

    # Use a dict with node_id as key to de-duplicate nodes
    all_nodes: dict[str, NodeWithScore] = {}

    # Accumulate scores for each node across all retrievers
    for nodes_with_scores in results.values():
        for nws in nodes_with_scores:
            node_id = nws.node.node_id
            if node_id in all_nodes:
                # Sum scores
                all_nodes[node_id].score += nws.score
                # Merge metadata (e.g. original scores from each retriever)
                all_nodes[node_id].node.metadata |= nws.node.metadata
            else:
                all_nodes[node_id] = nws

    # Divide by number of total number of result lists to get mean.
    # If node did not show up in a specific result list, its score is implicitly set to 0
    num_result_lists = len(results.keys())
    for nws in all_nodes.values():
        nws.score /= num_result_lists
    final_nodes = sorted(all_nodes.values(), key=lambda x: x.score or 0.0, reverse=True)
    ## Save Fusion Scores
    if save_score:
        save_score_to_node_metadata({"fusion_score": final_nodes})
    return final_nodes


def distribution_based_score_fusion(
    results: dict[str, list[NodeWithScore]],
    weights: dict[str, float] | None = None,
    save_score: bool = False,
    format_metadata_key: Callable[[str], str] | None = None,
) -> list[NodeWithScore]:
    """Apply distribution-based score fusion.

    Based on llama_index.core.retrievers.fusion_retriever.QueryFusionRetriever

    Args:
        results (Dict[str], List[NodeWithScore]]): Results from multiple
            queries and/or retrievers that we want to fuse.
            Keys identify a named result of nodes generated from a query and retriever.
            Values are lists of nodes with scores for that query and retriever.
        weights (Dict[str, float], optional): Weights for each named result list. Defaults to None.
            If None, then all retrievers are weighted equally.
        save_score (bool, optional): If true, saves original scores to node metadata.
        format_metadata_key (Callable, optional): A function that takes a string and
            returns a string. If provided, will be used to format the string key in
            results which will become the key in the node metadata dict. Defaults to None.

    Returns:
        List[NodeWithScore]: List with node scores generated by fusion method.
    """
    return relative_score_fusion(
        results=results,
        weights=weights,
        dist_based=True,
        save_score=save_score,
        format_metadata_key=format_metadata_key,
    )


def reciprocal_rerank_fusion(
    results: dict[str, list[NodeWithScore]],
    k: float = 60.0,
    save_score: bool = False,
    format_metadata_key: Callable[[str], str] | None = None,
) -> list[NodeWithScore]:
    """Apply reciprocol rank fusion. This method considers only the rank of the nodes
    in the result lists and does not consider the scores of the nodes.

    Based on llama_index.core.retrievers.fusion_retriever.QueryFusionRetriever

    The original paper uses k=60 for best results:
    https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf

    Args:
        results (Dict[str, List[NodeWithScore]]): Results from multiple
            queries and/or retrievers that we want to fuse.
            Keys identify a named result of nodes generated from a query and retriever.
            Values are lists of nodes with scores for that query and retriever.
        k (float, optional): `k` is a parameter used to control the impact of outlier rankings.
        save_score (bool, optional): If true, saves original scores to node metadata.
        format_metadata_key (Callable, optional): A function that takes a string and
            returns a string. If provided, will be used to format the string key in
            results which will become the key in the node metadata dict. Defaults to None.


    Returns:
        List[NodeWithScore]: List with node scores generated by fusion method.
    """
    ## Save Original Scores
    if save_score:
        r = results
        if format_metadata_key:
            r = {format_metadata_key(k): v for k, v in r.items()}
        r = {f"{k}_score": v for k, v in r.items()}
        save_score_to_node_metadata(r)

    fused_scores: dict[str, float] = {}
    all_nodes: dict[str, NodeWithScore] = {}

    # compute reciprocal rank scores
    for nodes_with_scores in results.values():
        for rank, nws in enumerate(
            sorted(nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True)
        ):
            node_id = nws.node.node_id
            all_nodes[node_id] = nws
            if node_id not in fused_scores:
                fused_scores[node_id] = 0.0
            fused_scores[node_id] += 1.0 / (rank + k)

    # sort results
    reranked_results = dict(sorted(fused_scores.items(), key=lambda x: x[1], reverse=True))

    # adjust node scores
    reranked_nodes: list[NodeWithScore] = []
    for node_id, score in reranked_results.items():
        reranked_nodes.append(all_nodes[node_id])
        reranked_nodes[-1].score = score

    ## Save Fusion Scores
    if save_score:
        save_score_to_node_metadata({"fusion_score": reranked_nodes})
    return reranked_nodes

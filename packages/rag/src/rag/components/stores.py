"""Components used in ingestion and query scripts and pipelines."""

import logging
import os
from collections.abc import Callable

from qdrant_client import AsyncQdrantClient, QdrantClient

from rag.components.models import get_embed_model
from rag.vector_stores import QdrantVectorStore, default_bge_m3_encoder

logger = logging.getLogger()


def get_vectorstore(
    collection_name: str = "Collection",
    on_disk_payload: bool = True,
    location: str | None = None,
    port: int | None = None,
    grpc_port: int | None = None,
    prefer_grpc: bool = False,
    upsert_batch_size: int = 100,
    query_batch_size: int = 10,
    num_query_workers: int = 16,
    enable_hybrid: bool = True,
    enable_dense: bool = True,
    enable_sparse: bool = True,
    enable_colbert: bool = False,
    query_encoder_fn: Callable | None = None,
    timeout: float = 60.0,
) -> QdrantVectorStore:
    """Get Handle to Qdrant Vectorstore.

    This is a customized version of the llama-index Qdrant Vectorstore which enables
    multivector hybrid search (e.g. multiple sparse and dense vectors representing
    each document and query).

    The Qdrant Vector database should be separately hosted and started as a service
    running in a docker container.

    Args:
        collection_name (str, optional): Qdrant collection name. Defaults to "TestTextSource".
        on_disk_payload (bool, optional): Whether Qdrant collection payloads are stored
            on disk (rather than in memory). The index and metadata used for query filtering
            are always stored in-memory even if payloads are stored on disk. Storing payloads
            on disk reduces memory consumption for very large vector datastores with slight
            increase in retrieval latency. This configuraiton must be set at the time
            of collection creation. Defaults to True.
        location (str, optional): Location for Qdrant Client.
        port (int, optional): Qdrant port. Defaults to 6333.
        grpc_port (int, optional): Qdrant grpc port. Defaults to 6334.
        prefer_grpc (bool, optional): Whether to prefer grpc. Defaults to False.
        upsert_batch_size (int, optional): Batch size for upserting points into database.
            Defaults to 100.
        query_batch_size (int, optional): Batch size for querying points from database.
            Defaults to 10.
        num_query_workers (int, optional): Number of workers for querying points from database.
            Defaults to 16.
        enable_hybrid (bool, optional): Whether to use hybrid search/retrieval. Hybrid
            retrieval must be set at time of data ingestion in order to support hybrid
            search queries. Defaults to True.
        enable_dense (bool, optional): Whether to enable dense embedding vectors.
            Defaults to True.
        enable_sparse (bool, optional): Whether to enable sparse embedding vectors.
            Defaults to True.
        enable_colbert (bool, optional): Whether to enable colbert embedding vectors.
            Colbert vectors are multivector representations of text (as opposed to
            single vector representations for dense and sparse embeddings). Currently
            colbert embeddings can be generated by BGE-M3, but is not a supported
            storage type in Qdrant, so we currently cannot support it. Defaults to False.
        query_encoder_fn (Callable | None, optional): Function used to embed query text.
            If None, default is used.
        timeout (float, optional): Timeout for API calls in seconds. Defaults to 60.0.
            NOTE: Default Qdrant Client timeout is 5 seconds.

    Returns:
        QdrantVectorStore: Llama-index wrapper around Qdrant Vector Database
            which is further customized to support BGE-M3 embeddings for multiple
            embedding/hybrid retrieval.
    """
    location = location or os.environ["QDRANT_HOST"]
    port = port or os.environ["QDRANT_HOST_PORT"]
    grpc_port = grpc_port or os.environ["QDRANT_GRPC_HOST_PORT"]
    query_encoder_fn = query_encoder_fn or default_bge_m3_encoder(
        embed_model=get_embed_model(timeout=timeout)
    )
    # Initialize Qdrant Client
    client = QdrantClient(
        location=location, port=port, grpc_port=grpc_port, prefer_grpc=prefer_grpc, timeout=timeout
    )
    aclient = AsyncQdrantClient(
        location=location, port=port, grpc_port=grpc_port, prefer_grpc=prefer_grpc, timeout=timeout
    )
    # Create Vectorstore (llama-index Qdrant Vectorstore customized for BGE-M3 compatibility)
    vector_store = QdrantVectorStore(
        collection_name=collection_name,
        on_disk_payload=on_disk_payload,
        client=client,
        aclient=aclient,
        upsert_batch_size=upsert_batch_size,
        query_batch_size=query_batch_size,
        num_query_workers=num_query_workers,
        enable_hybrid=enable_hybrid,
        enable_dense=enable_dense,
        enable_sparse=enable_sparse,
        enable_colbert=enable_colbert,
        query_encoder_fn=query_encoder_fn,
    )
    return vector_store

"""The score report generated by the Judge."""

import asyncio
import warnings
from collections.abc import Awaitable
from typing import Self

import pandas as pd
from llama_index.llms.openai import OpenAI
from pydantic import Field
from pydantic_utils import LLM, LLMBaseModel
from rag import CLAIM_NODE, SENTENCE_NODE
from rag.llms.openai_like import OpenAILike
from utils import load_pandas, save_pandas

from llm_judge.prompts import prompt_summarize_reasons
from llm_judge.schema import ExplanationSummary, Reasons, Verdict


class ScoreReport(LLMBaseModel):
    """A ScoreReport is the aggregate bundle of ratings for all propositions from a single
    text input being made by a single rater."""

    # Scores
    supported_score: float = Field(default=0.0, description="Percentage of supported texts.")
    not_supported_score: float = Field(
        default=0.0, description="Percentage of not supported texts."
    )
    not_addressed_score: float = Field(
        default=0.0, description="Percentage of not addressed texts."
    )
    # Summary Explanation for Scores
    supported_explanation: str = Field(default="", description="Reasoning for supported texts.")
    not_supported_explanation: str = Field(
        default="", description="Reasoning for not supported texts."
    )
    not_addressed_explanation: str = Field(
        default="", description="Reasoning for not addressed texts."
    )
    # Raw Counts
    supported_count: int = Field(default=0, description="Number of supported texts.")
    not_supported_count: int = Field(default=0, description="Number of not supported texts.")
    not_addressed_count: int = Field(default=0, description="Number of not addressed texts.")
    total_count: int = Field(default=0, description="Total number of texts.")
    # Raw Verdicts & Reasons
    verdicts: list[Verdict] = Field(
        default_factory=list, description="List of individual verdicts."
    )
    supported: list[Verdict] = Field(
        default_factory=list, description="List of supported verdicts."
    )
    not_supported: list[Verdict] = Field(
        default_factory=list, description="List of not supported verdicts."
    )
    not_addressed: list[Verdict] = Field(
        default_factory=list, description="List of not addressed verdicts."
    )
    # Proposition & EHR Fact Types
    proposition_type: str = Field(
        str="", description=f"Proposition type (node kind): {CLAIM_NODE}, {SENTENCE_NODE}."
    )
    fact_type: str = Field(
        str="", description=f"EHR Fact type (node kind): {CLAIM_NODE}, {SENTENCE_NODE}."
    )
    # Judge Configuration
    judge_config: dict | None = Field(
        default=None, description="Judge configuration used to generate the ScoreReport."
    )

    @classmethod
    def class_name(cls) -> str:
        return "ScoreReport"

    @classmethod
    def from_defaults(
        cls,
        verdicts: list[Verdict],
        include_explanations: bool = True,
        llm: OpenAILike | OpenAI | None = None,
        num_workers: int = 8,
        num_invalid_output_retries: int = 3,
        proposition_type: str | None = None,
        fact_type: str | None = None,
        judge_config: dict | None = None,
    ) -> "ScoreReport":
        """Compute scores and explanations from list of verdicts."""
        obj = cls.create(
            verdicts=verdicts,
            llm=llm,
            num_workers=num_workers,
            num_invalid_output_retries=num_invalid_output_retries,
            proposition_type=proposition_type,
            fact_type=fact_type,
            judge_config=judge_config,
        )
        # Generate Explanations
        if obj and include_explanations:
            obj = obj.generate_explanations()
        return obj

    @classmethod
    async def a_from_defaults(
        cls,
        verdicts: list[Verdict],
        include_explanations: bool = True,
        llm: OpenAILike | OpenAI | None = None,
        num_workers: int = 8,
        num_invalid_output_retries: int = 3,
        proposition_type: str | None = None,
        fact_type: str | None = None,
        judge_config: dict | None = None,
    ) -> Awaitable["ScoreReport"]:
        """Compute scores and explanations from list of verdicts."""
        obj = cls.create(
            verdicts=verdicts,
            llm=llm,
            num_workers=num_workers,
            num_invalid_output_retries=num_invalid_output_retries,
            proposition_type=proposition_type,
            fact_type=fact_type,
            judge_config=judge_config,
        )
        if obj and include_explanations:
            obj = await obj.a_generate_explanations()
        return obj

    @classmethod
    def create(
        cls,
        verdicts: list[Verdict],
        llm: OpenAILike | OpenAI | None = None,
        num_workers: int = 8,
        num_invalid_output_retries: int = 3,
        proposition_type: str | None = None,
        fact_type: str | None = None,
        judge_config: dict | None = None,
    ) -> "ScoreReport":
        if not verdicts:
            warnings.warn("No verdicts provided. Returning `None` object.")
            return None
        # Sort verdicts into categories
        supported, not_supported, not_addressed = [], [], []
        for v in verdicts:
            match v.verdict:
                case "Supported":
                    supported.append(v)
                case "Not Supported":
                    not_supported.append(v)
                case "Not Addressed":
                    not_addressed.append(v)
                case _:
                    raise ValueError(f"Invalid verdict: {v.verdict}")

        supported_count = len(supported)
        not_supported_count = len(not_supported)
        not_addressed_count = len(not_addressed)
        total_count = len(verdicts)

        # Compute scores
        supported_score = float(supported_count) / total_count
        not_supported_score = float(not_supported_count) / total_count
        not_addressed_score = float(not_addressed_count) / total_count

        # Create ScoreReport Object
        return cls(
            supported_score=supported_score,
            not_supported_score=not_supported_score,
            not_addressed_score=not_addressed_score,
            supported_count=supported_count,
            not_supported_count=not_supported_count,
            not_addressed_count=not_addressed_count,
            total_count=total_count,
            verdicts=verdicts,
            supported=supported,
            not_supported=not_supported,
            not_addressed=not_addressed,
            llm=llm.llm if isinstance(llm, LLM) else llm,
            num_workers=num_workers,
            num_invalid_output_retries=num_invalid_output_retries,
            proposition_type=proposition_type,
            fact_type=fact_type,
            judge_config=judge_config,
        )

    def generate_explanations(self) -> Self:
        """Generate explanations for supported, not_supported, and not addressed texts."""
        supported_reasons = [v.reason for v in self.supported]
        not_supported_reasons = [v.reason for v in self.not_supported]
        not_addressed_reasons = [v.reason for v in self.not_addressed]
        supported_json_str = Reasons(reasons=supported_reasons).model_dump_json(indent=4)
        not_supported_json_str = Reasons(reasons=not_supported_reasons).model_dump_json(indent=4)
        not_addressed_json_str = Reasons(reasons=not_addressed_reasons).model_dump_json(indent=4)

        supported_explanation = self._generate(
            json=supported_json_str,
            label="Supported",
            prompt=prompt_summarize_reasons,
            response_format=ExplanationSummary,
        )
        not_supported_explanation = self._generate(
            json=not_supported_json_str,
            label="Not Supported",
            prompt=prompt_summarize_reasons,
            response_format=ExplanationSummary,
        )
        not_addressed_explanation = self._generate(
            json=not_addressed_json_str,
            label="Not Addressed",
            prompt=prompt_summarize_reasons,
            response_format=ExplanationSummary,
        )
        self.supported_explanation = supported_explanation.summary if supported_explanation else ""
        self.not_supported_explanation = (
            not_supported_explanation.summary if not_supported_explanation else ""
        )
        self.not_addressed_explanation = (
            not_addressed_explanation.summary if not_addressed_explanation else ""
        )
        return self

    async def a_generate_explanations(self) -> Awaitable[Self]:
        """Generate explanations for supported, not supported, and not addressed texts."""
        supported_reasons = [v.reason for v in self.supported]
        not_supported_reasons = [v.reason for v in self.not_supported]
        not_addressed_reasons = [v.reason for v in self.not_addressed]
        supported_json_str = Reasons(reasons=supported_reasons).model_dump_json(indent=4)
        not_supported_json_str = Reasons(reasons=not_supported_reasons).model_dump_json(indent=4)
        not_addressed_json_str = Reasons(reasons=not_addressed_reasons).model_dump_json(indent=4)
        jobs = [
            self._a_generate(
                json=supported_json_str,
                label="Supported",
                prompt=prompt_summarize_reasons,
                response_format=ExplanationSummary,
            ),
            self._a_generate(
                json=not_supported_json_str,
                label="Not Supported",
                prompt=prompt_summarize_reasons,
                response_format=ExplanationSummary,
            ),
            self._a_generate(
                json=not_addressed_json_str,
                label="Not Addressed",
                prompt=prompt_summarize_reasons,
                response_format=ExplanationSummary,
            ),
        ]
        (
            supported_explanation,
            not_supported_explanation,
            not_addressed_explanation,
        ) = await asyncio.gather(*jobs)
        self.supported_explanation = supported_explanation.summary if supported_explanation else ""
        self.not_supported_explanation = (
            not_supported_explanation.summary if not_supported_explanation else ""
        )
        self.not_addressed_explanation = (
            not_addressed_explanation.summary if not_addressed_explanation else ""
        )
        return self

    def __str__(self) -> str:
        return (
            f"ScoreReport(supported_score={self.supported_score:.2f}, "
            f"not_supported_score={self.not_supported_score:.2f}, "
            f"not_addressed_score={self.not_addressed_score:.2f}, "
            f"supported_count={self.supported_count}, "
            f"not_supported_count={self.not_supported_count}, "
            f"not_addressed_count={self.not_addressed_count}, "
            f"Judge Configuration={self.judge_config})"
        )

    def __repr__(self) -> str:
        return self.__str__()

    def report(self) -> str:
        return (
            f"Supported Score: {self.supported_score:.2f}\n"
            f"Not Supported Score: {self.not_supported_score:.2f}\n"
            f"Not Addressed Score: {self.not_addressed_score:.2f}\n"
            f"Supported Count: {self.supported_count}\n"
            f"Not Supported Count: {self.not_supported_count}\n"
            f"Not Addressed Count: {self.not_addressed_count}\n"
            f"Total Count: {self.total_count}\n"
            f"Supported Explanation: {self.supported_explanation}\n"
            f"Not Supported Explanation: {self.not_supported_explanation}\n"
            f"Not Addressed Explanation: {self.not_addressed_explanation}\n"
            f"Proposition Type: {self.proposition_type}\n"
            f"Fact Type: {self.fact_type}\n"
            f"Judge Configuration: {self.judge_config}"
        )

    def to_pandas(self) -> pd.DataFrame:
        """Convert the ScoreReport to a Pandas DataFrame."""
        return (
            pd.Series(self.model_dump())
            .loc[
                [
                    "supported_score",
                    "not_supported_score",
                    "not_addressed_score",
                    "supported_count",
                    "not_supported_count",
                    "not_addressed_count",
                    "total_count",
                    "supported_explanation",
                    "not_supported_explanation",
                    "not_addressed_explanation",
                    "verdicts",
                    "supported",
                    "not_supported",
                    "not_addressed",
                    "proposition_type",
                    "fact_type",
                    "judge_config",
                ]
            ]
            .to_frame()
            .T
        )

    def save(self, filepath: str, **kwargs) -> None:
        """Save the ScoreReport to disk.
        This method converts the ScoreReport object to a Pandas DataFrame
        and then saves the attributes to disk.
        """
        df = self.to_pandas()
        save_pandas(df=df, filepath=filepath, **kwargs)

    @classmethod
    def load(cls, filepath: str, **kwargs) -> "ScoreReport":
        """Load a ScoreReport from disk."""
        df = load_pandas(filepath)
        return cls(**df.squeeze().to_dict(), **kwargs)
